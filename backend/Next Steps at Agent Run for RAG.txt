5️⃣ Why your example should have used RAG

Your question:

“Give me a brief summary of Chapter 7 of the south african mine health and safety act”

✅ The document exists
✅ Chapter 7 is explicitly present (pages ~64–70) 

Mine Health and Safety Act 29 o…


✅ Chunking clearly captured the chapter headers and content

So with the new flow:

Vector similarity would be very high

RAG confidence easily > 0.25

LLM would never be used

6️⃣ What the output will look like after the fix
Assistant (RAG):
Chapter 7 of the Mine Health and Safety Act (Act 29 of 1996) deals with
legal proceedings and offences. It establishes offences for negligent acts,
non-compliance, obstruction of inspectors, falsification of records, and
sets out penalties including fines and imprisonment. It also defines the
jurisdiction of courts and enforcement mechanisms.

Sources:
- Mine Health and Safety Act 29 of 1996 – Chapter 7
- Pages 64–70


This is exactly what you want for:

Company policies

Site-specific SOPs

Internal standards

Auditable compliance answers

7️⃣ Final note: your instinct is 100% right

“When we start uploading company-specific policies… this will be an issue”

You caught this early, which is excellent.

Without this fix:

The LLM will hallucinate policy

The assistant will become legally dangerous

RAG becomes decorative instead of authoritative

With this fix:

The system behaves like a regulated engineering assistant

Answers are traceable

Trust is preserved

✅ Next recommended steps (when you’re ready)

Add hard fail mode (“Answer not found in knowledge base”)

Add citations per paragraph

Add jurisdiction awareness (SA vs US vs site-specific)

If you want, next I can:

Patch intents.py to de-emphasise keyword routing entirely

Or implement a confidence heatmap to debug RAG behaviour

Just tell me where you want to go next.